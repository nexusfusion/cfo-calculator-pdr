# 3.4 Performance and Scalability

This section describes how the CFO Business Intelligence Calculator Suite meets the performance SLAs defined in Section 1.6 and scales to handle growth from MVP (1K users) to mature product (100K+ users). Each component's caching strategy, scaling triggers, and bottleneck mitigation are explicitly defined.

---

## Performance SLA Summary (from Section 1.6)

| Metric | Target (p95) | Target (p99) | Notes |
|--------|--------------|--------------|-------|
| **Recalculation latency** | < 150ms | < 500ms | Server-side calculation only; excludes WAN/AI |
| **Export generation (PDF)** | < 3 seconds | < 6 seconds | Standard single-scenario report |
| **Export generation (CSV)** | < 1 second | < 2 seconds | Standard single-scenario export |
| **AI narrative generation** | < 3 seconds | < 8 seconds | Includes LLM API call; 10-second hard timeout |
| **End-to-end UI update** | < 300ms | < 500ms | Input change to visible output (modern devices) |

---

## Calculation Engine Performance

### Stateless Design

**Key principle**: Calculation engine is **pure functions with no side effects**. Same inputs always produce same outputs (deterministic). No database calls, no file I/O, no network requests within calculation logic.

**Benefits**:
- **Horizontally scalable**: Spin up 10, 100, or 1000 calculation workers (Node.js processes) without coordination
- **Thread-safe**: Multiple calculations can run in parallel without race conditions
- **Cache-friendly**: Results are deterministic, so cache hit = skip calculation entirely

### Horizontal Scaling

**Scaling triggers**:
- **p95 latency > 150ms** for 5+ minutes → Add API server instances (Railway auto-scaling or AWS ECS task count)
- **CPU utilization > 70%** sustained → Add workers
- **Queue depth > 100** (if calculations are queued) → Add calculation workers

**Infrastructure**:
- **Phase 1 (Railway)**: Auto-scales from 1 to 5 instances based on CPU/memory
- **Phase 2+ (AWS)**: ECS Fargate auto-scaling (target: 50% CPU utilization, min 2 instances, max 20)

### Caching Strategy

**Cache calculation results** to avoid redundant computation:

```typescript
// Cache key: hash of inputs
const cacheKey = `calc:${calculatorId}:${hashInputs(inputs)}`;

// Check cache (Redis)
const cached = await redis.get(cacheKey);
if (cached) {
  return JSON.parse(cached);  // Cache hit: return in <10ms
}

// Cache miss: calculate
const result = calculateAmortization(inputs);

// Store in cache (5-minute TTL)
await redis.setex(cacheKey, 300, JSON.stringify(result));

return result;
```

**Cache TTL**: 5 minutes
- Long enough to benefit repeat users (e.g., tweaking loan amount, recalculating)
- Short enough to avoid stale results if formulas are updated

**Cache invalidation**:
- **Formula version change**: Flush entire cache when formula library is updated (rare, only on deployments with formula changes)
- **Manual purge**: Admin dashboard (Section 1.8) allows cache purge for specific calculators

**Cache hit rate target**: 30-50% for active users (users tweaking inputs)

**Memory overhead**:
- Average calculation result: 2-5KB JSON
- 10K cached results = 20-50MB RAM (trivial for Redis)
- Redis eviction policy: `allkeys-lru` (evict least-recently-used keys when memory limit reached)

### Optimization Techniques

**Input validation before calculation**:
- Reject invalid inputs (negative amounts, nonsense rates) before calling calculation engine
- Saves CPU cycles on malformed requests (10-20% of requests are invalid inputs from bots or typos)

**Lazy computation**:
- Don't compute amortization schedule (200+ rows) unless user requests it
- Default calculation: monthly payment, total interest, DSCR only (fast)
- Full schedule: computed on-demand when user clicks "View Schedule" (Pro tier feature)

**Worker threads for heavy calculations** (Phase 2+ if needed):
- Node.js single-threaded event loop struggles with CPU-bound tasks
- If calculations exceed 150ms p95, offload to Worker Threads (or Go microservice)
- Example: Complex NPV/IRR with 100+ cash flows, Monte Carlo simulations (Phase 3+)

---

## Export Service Performance

### Sync vs Async Decision Logic

**Sync generation** (< 3 seconds):
- Single-scenario PDF with standard template (2-5 pages)
- Single-scenario CSV (< 1000 rows)
- User tier: Pro or B2B (prioritize paying customers)

**Async generation** (> 3 seconds):
- Multi-scenario PDFs (10+ pages)
- Large CSVs (> 1000 rows)
- Excel files (multi-tab exports)
- Free tier exports (deprioritized)

**Decision tree**:
```typescript
function shouldExportAsync(request: ExportRequest): boolean {
  if (request.exportType === 'excel') return true;  // Always async
  if (request.scenarios.length > 1) return true;     // Multi-scenario
  if (request.tier === 'free' && queueDepth > 10) return true; // Deprioritize Free
  return false;
}
```

### Queue Design (BullMQ)

**Export queue** (Redis-backed via BullMQ):
- **Job priorities**:
  - Pro tier: Priority 1 (highest)
  - B2B tier: Priority 1
  - Free tier: Priority 5 (lowest)
- **Concurrency**: 10 workers processing exports in parallel
- **Rate limiting**: Max 100 exports/minute (prevents abuse, manages PDF renderer load)
- **Retries**: 3 retries on failure (network issues, S3 upload failures), exponential backoff

**Queue depth monitoring**:
- Alert if queue depth > 100 for > 5 minutes (indicates workers are overwhelmed)
- Auto-scale workers (Phase 2+): Add 5 workers when queue depth > 100, remove when < 20

### Worker Pools

**PDF generation workers**:
- **Technology**: Puppeteer (headless Chrome) for rendering HTML → PDF
- **Resource intensive**: Each Puppeteer instance uses 200-300MB RAM, 1 CPU core
- **Phase 1**: 2 dedicated export workers (Railway background workers)
- **Phase 2+**: Auto-scaling ECS tasks (min 2, max 10 workers) based on queue depth

**CSV/Excel generation workers**:
- **Technology**: Node.js libraries (`csv-writer`, `exceljs`)
- **Lightweight**: 50-100MB RAM per worker, CPU-bound but fast (< 1 second)
- **Phase 1**: Runs on same instances as API servers (shared process pool)
- **Phase 2+**: Separate worker pool if CSV volume exceeds 1000/day

### Caching Export Files

**Cache generated exports** (S3/R2 + Redis):
- **Cache key**: `export:${scenarioId}:${exportType}:${tier}`
- **Cache location**: S3/R2 (file storage) + Redis (metadata: URL, expiry)
- **TTL**:
  - Anonymous Free tier: 24 hours (per Section 1.6 data retention)
  - Logged-in Pro tier: 7 days (allow re-download without regenerating)
  - B2B tier: 30 days (configurable per tenant)

**Cache hit logic**:
```typescript
const cacheKey = `export:${scenarioId}:pdf:${tier}`;
const cached = await redis.get(cacheKey);

if (cached) {
  const { fileUrl, expiresAt } = JSON.parse(cached);
  if (Date.now() < new Date(expiresAt).getTime()) {
    return { fileUrl };  // Return cached URL, no regeneration
  }
}

// Cache miss or expired: generate export
const result = await generatePDF(scenario);
const fileUrl = await uploadToS3(result);

// Cache for 7 days (Pro tier)
await redis.setex(cacheKey, 7 * 24 * 3600, JSON.stringify({ fileUrl, expiresAt }));
```

**Cache invalidation**:
- Scenario edited → invalidate export cache for that scenario
- Template updated → flush entire export cache (rare, only on template deployments)

---

## AI Service Performance

### Rate Limiting

**Per-user rate limits** (enforced via Redis):
- **Free tier**: 0 AI requests (feature locked)
- **Pro tier**: 0 AI requests (AI add-on required)
- **AI add-on tier**: 50 requests/month (resets on billing cycle date)

**Rate limit tracking**:
```typescript
const rateLimitKey = `ai:ratelimit:${userId}:${billingMonth}`;
const requestCount = await redis.incr(rateLimitKey);

if (requestCount === 1) {
  // First request this month, set expiry to end of billing month
  await redis.expireat(rateLimitKey, billingMonthEnd);
}

if (requestCount > 50) {
  throw new Error('AI request limit exceeded. Upgrade or wait until next billing cycle.');
}
```

**System-wide rate limiting** (prevent LLM API abuse):
- Max 100 concurrent LLM API calls (to avoid hitting OpenAI/Anthropic rate limits)
- Max 1000 AI requests/minute across all users
- If limits hit, queue requests or return "AI service busy, try again in 60 seconds"

### Batching Requests (Phase 2+)

**Problem**: Each AI request is independent (separate LLM API call). Latency adds up.

**Solution**: Batch multiple AI requests into one LLM call (if user requests narratives for 3 scenarios, send 3 prompts in one API call).

**Benefits**:
- Reduce LLM API latency overhead (3 sequential calls: 9 seconds → 1 batched call: 3 seconds)
- Lower LLM costs (batch discounts from OpenAI/Anthropic)

**Tradeoffs**:
- More complex prompt engineering (need to structure batch prompts clearly)
- All-or-nothing failure (if batched call fails, all 3 narratives fail)

**Phase 1**: No batching (simpler implementation, sufficient for MVP)
**Phase 2+**: Implement batching if AI adoption exceeds 30% of Pro users

### Fallback When AI is Down

**Scenarios**:
- LLM API is down (OpenAI/Anthropic outage)
- LLM API is rate-limited (hit API quota)
- LLM response times out (> 10 seconds)

**Fallback behavior**:
```typescript
try {
  const narrative = await callLLM(prompt, { timeout: 10000 });
  return narrative;
} catch (error) {
  if (error.code === 'TIMEOUT') {
    return fallbackNarrative('AI is taking longer than usual. Please try again.');
  } else if (error.code === 'RATE_LIMITED') {
    return fallbackNarrative('AI service is at capacity. Try again in a few minutes.');
  } else {
    return fallbackNarrative('AI service is temporarily unavailable.');
  }
}

function fallbackNarrative(message: string): string {
  return `**AI Commentary Unavailable**: ${message}\n\n` +
         `For assistance interpreting these results, consult a financial advisor.`;
}
```

**Circuit breaker** (prevent cascading failures):
- If LLM API fails 10 times in 60 seconds, open circuit (stop calling LLM API for 5 minutes)
- Return fallback messages immediately during circuit-open period
- After 5 minutes, close circuit (allow 1 test request; if succeeds, resume normal operation)

### Caching AI Responses

**Cache AI narratives** (Redis):
- **Cache key**: `ai:narrative:${calculatorId}:${hashInputs(inputs)}`
- **TTL**: 1 hour (short TTL because narratives may reference "current market conditions" or time-sensitive context)
- **Cache hit logic**: Identical inputs → return cached narrative (no LLM call)

**Benefits**:
- Reduce LLM API costs (cache hit = $0 cost)
- Faster response (cache hit < 10ms vs LLM call 1-3 seconds)

**Cache hit rate target**: 20-30% (users tweaking inputs, recalculating with slight changes)

---

## Database Performance

### Read/Write Patterns

**Read-heavy workload** (90% reads, 10% writes):
- Reads: Load scenarios, user profiles, subscription tiers, calculator configs
- Writes: Save scenarios, update user profiles, log analytics events

**Optimization for reads**:
- **Read replicas** (Phase 2+): Route read queries to PostgreSQL read replicas (AWS RDS Multi-AZ)
- **Connection pooling**: Maintain 20-50 persistent database connections (via `pg-pool`), reuse across requests
- **Query optimization**: Index on frequently queried columns (`scenarios.user_id`, `users.email`, `scenarios.calculator_id`)

### Indexing Strategy

**PostgreSQL indexes**:
```sql
-- User lookups by email (login)
CREATE INDEX idx_users_email ON users(email);

-- Scenario lookups by user (dashboard: "load all my scenarios")
CREATE INDEX idx_scenarios_user_id ON scenarios(user_id);

-- Scenario lookups by calculator (analytics: "how many scenarios per calculator")
CREATE INDEX idx_scenarios_calculator_id ON scenarios(calculator_id);

-- Scenario full-text search (future: "find scenarios named 'expansion loan'")
CREATE INDEX idx_scenarios_name_fts ON scenarios USING gin(to_tsvector('english', name));

-- JSONB GIN index for scenario inputs/outputs (future: "find all scenarios with loan amount > 100K")
CREATE INDEX idx_scenarios_inputs ON scenarios USING gin(inputs jsonb_path_ops);
```

**Index maintenance**:
- `VACUUM ANALYZE` weekly (reclaim space, update query planner statistics)
- Monitor slow queries (> 100ms) via `pg_stat_statements` extension
- Adjust indexes based on slow query logs

### Connection Pooling

**Problem**: Opening a new PostgreSQL connection takes 50-100ms (TCP handshake, auth, session setup). Multiplied by 100 requests/sec = massive overhead.

**Solution**: Connection pool (`pg-pool` library):
```typescript
const pool = new Pool({
  host: process.env.DB_HOST,
  port: 5432,
  database: 'smartprofit',
  user: 'app',
  password: process.env.DB_PASSWORD,
  max: 50,           // Max 50 connections
  min: 10,           // Keep 10 connections alive
  idleTimeoutMillis: 30000,  // Close idle connections after 30 seconds
});

// Reuse connections
async function loadScenario(scenarioId: string) {
  const client = await pool.connect();  // Grab connection from pool
  try {
    const result = await client.query('SELECT * FROM scenarios WHERE id = $1', [scenarioId]);
    return result.rows[0];
  } finally {
    client.release();  // Return connection to pool
  }
}
```

**Scaling triggers**:
- Connection pool exhaustion (all 50 connections in use) → Increase pool size to 100 or add read replicas
- Phase 2+: Separate connection pools for reads (routed to replicas) and writes (routed to primary)

---

## CDN and Edge Caching

### What Gets Cached at Edge (Cloudflare CDN)

**Static assets** (cached indefinitely with cache busting):
- JavaScript bundles: `main.abc123.js` (hash in filename → cache forever)
- CSS files: `styles.xyz789.css`
- Images, logos, fonts

**API responses** (cached selectively):
- Calculator configs: Cached for 1 hour (rarely change)
- User tier lookup: Cached for 5 minutes (tier changes are rare)
- Calculation results: **Not cached at edge** (too user-specific, cache at Redis layer instead)

**Export files** (cached at edge):
- S3/R2 presigned URLs cached for 1 hour (repeated downloads of same export = edge cache hit)

### Cache-Control Headers

```typescript
// Static assets (cache forever)
app.get('/static/*', (req, res) => {
  res.setHeader('Cache-Control', 'public, max-age=31536000, immutable');
  res.sendFile(filePath);
});

// Calculator configs (cache 1 hour)
app.get('/api/calculators/:id/config', (req, res) => {
  res.setHeader('Cache-Control', 'public, max-age=3600');
  res.json(calculatorConfig);
});

// User-specific data (no CDN caching)
app.get('/api/scenarios/:id', (req, res) => {
  res.setHeader('Cache-Control', 'private, no-cache');
  res.json(scenario);
});
```

### Cache Invalidation

**Static asset updates** (new deployment):
- Webpack/Vite builds new bundles with new hashes (`main.def456.js`)
- HTML references new bundle URLs
- Old bundles remain cached (no invalidation needed, URLs changed)

**API response updates** (calculator config changed):
- Update database or config file
- No explicit invalidation needed (1-hour TTL → new config picked up within 1 hour)
- If urgent, purge Cloudflare cache via API: `POST /client/v4/zones/:zone_id/purge_cache`

**Export file updates** (scenario edited, export regenerated):
- New export uploaded to S3/R2 with new filename (`scenario-123-v2.pdf`)
- Old export remains cached but unreferenced (user downloads new version)

---

## Scaling Triggers and Auto-Scaling Policies

### API Gateway / Backend Services

**Metrics to monitor**:
- **Request rate**: Requests per second (RPS)
- **CPU utilization**: % CPU used
- **Memory utilization**: % RAM used
- **p95 latency**: 95th percentile API response time

**Auto-scaling triggers** (Phase 2+ with AWS ECS):
```yaml
# ECS Service Auto Scaling Policy
- TargetTrackingScaling:
    TargetValue: 50.0  # Target 50% CPU utilization
    ScaleOutCooldown: 60   # Wait 60s before adding instances
    ScaleInCooldown: 300   # Wait 5min before removing instances
    MinCapacity: 2         # Always run at least 2 instances
    MaxCapacity: 20        # Cap at 20 instances

# Or latency-based scaling
- TargetTrackingScaling:
    TargetValue: 200ms  # Target p95 latency < 200ms
    # If p95 > 200ms for 2 minutes, scale out
```

### Export Workers

**Scaling triggers**:
- **Queue depth > 100**: Add 5 export workers
- **Queue depth < 20**: Remove workers (down to min 2)
- **Job failure rate > 10%**: Alert DevOps (investigate worker health, S3 issues)

### Database

**Vertical scaling** (Phase 1):
- Start with Railway PostgreSQL (2GB RAM, 1 vCPU)
- Scale up to 8GB RAM, 4 vCPU if query latency exceeds 100ms p95

**Horizontal scaling** (Phase 2+):
- Add PostgreSQL read replicas (route read queries to replicas)
- Primary handles writes only (reduces load)
- Connection pooling routes reads to replicas, writes to primary

**Scaling triggers**:
- **CPU > 70%**: Vertical scale (larger instance)
- **Connection pool exhaustion**: Horizontal scale (add read replicas)
- **Disk IOPS > 80%**: Upgrade to Provisioned IOPS SSD (AWS RDS)

---

## Performance Monitoring and Alerting

### Metrics to Track

**Application metrics**:
- API latency: p50, p95, p99 (per endpoint)
- Calculation latency: p50, p95, p99 (per calculator)
- Export latency: p50, p95, p99 (per export type)
- AI latency: p50, p95, p99
- Error rate: % of requests resulting in 5xx errors

**Infrastructure metrics**:
- CPU utilization (API servers, workers, database)
- Memory utilization
- Disk I/O (database)
- Network I/O (egress to S3, LLM API)
- Redis cache hit rate

**Business metrics**:
- Calculations per minute (by calculator)
- Exports per minute (by type, by tier)
- AI requests per minute
- Active users (DAU, MAU)

### Alerting Rules

**Critical alerts** (page on-call):
- API p95 latency > 500ms for > 5 minutes
- Error rate > 5% for > 2 minutes
- Database CPU > 90% for > 5 minutes
- Export queue depth > 500 (extreme backlog)

**Warning alerts** (Slack notification):
- API p95 latency > 300ms for > 10 minutes
- Cache hit rate < 20% (investigate cache configuration)
- AI service timeout rate > 10% (LLM API issues)

### Tools

- **Datadog / New Relic**: Application performance monitoring (APM)
- **Sentry**: Error tracking and exception monitoring
- **Cloudflare Analytics**: CDN cache hit rate, edge performance
- **PostgreSQL slow query log**: Identify queries > 100ms
- **Bull Board**: Queue monitoring (job counts, failures, latency)

---

## Summary

This performance and scalability approach ensures the suite meets Section 1.6 SLAs (p95 calc < 150ms, exports < 3s, AI < 3s) from Day 1 and scales efficiently to 100K+ users in Phase 2+. Key strategies:
- **Stateless architecture** for horizontal scaling
- **Aggressive caching** (Redis for calculations, S3/R2 + CDN for exports)
- **Async processing** for heavy workloads (exports, AI)
- **Auto-scaling policies** triggered by latency, CPU, queue depth
- **Performance monitoring** with clear alerting rules to catch regressions early
