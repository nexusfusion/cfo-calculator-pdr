# 9.2 Workstream Mapping

This section defines the engineering workstreams for building the CFO Business Intelligence Calculator Suite, including team ownership, dependencies, key deliverables, timelines, and workstream-specific risks. Workstreams run in parallel where possible to accelerate delivery.

---

## Workstream Overview

**Total workstreams**: 6 core workstreams

**Parallelization strategy**:
- **Foundation workstreams** (Calculation Engine, UI/UX, Export, Analytics) run in parallel during M0
- **AI Integration** starts after calculation engine is stable (Week 8+)
- **B2B/White-Label** starts after core platform is validated (M2)

**Team size assumption**: 5-8 engineers (2 backend, 2 frontend, 1 full-stack, 1 data/analytics, 1 designer, 1 PM)

---

## 1. Calculation Engine & Formula Library

### Team/Owner

**Team**:
- Backend engineering lead (owner)
- Senior backend engineer (contributor)
- QA engineer (testing support)

**Total capacity**: 2.5 FTEs (full-time equivalents)

### Dependencies

**None** (foundation for everything else)

**Critical path**: This workstream blocks calculator development. Must be complete before M1.

### Key Deliverables

**1. Formula library structure and versioning**

**Implementation**:
- Each formula is a TypeScript function with version number
- Formula files organized by calculator type:
  ```
  /src/formulas/
    lending/
      calculateDSCR_v1.ts
      calculateAmortization_v1.ts
      calculateSBA7a_v1.ts
    cashflow/
      calculateRunway_v1.ts
      calculateBurnRate_v1.ts
    profitability/
      calculateBreakeven_v1.ts
      calculateMargin_v1.ts
    valuation/
      calculateDCF_v1.ts
      calculateMultiples_v1.ts
  ```

- Version control:
  - When formula changes, create new version (e.g., `calculateDSCR_v2.ts`)
  - Old version remains available (support historical calculations)
  - Results include version stamp

**2. Input validation and sanitization**

**Validation rules**:
- **Type validation**: Ensure all inputs are numbers (not strings, nulls, undefined)
- **Range validation**: Enforce min/max values (e.g., interest rate 0-30%, loan term 1-360 months)
- **Business logic validation**: Ensure inputs make sense (e.g., monthly payment cannot exceed loan amount)

**Sanitization**:
- Remove commas from currency inputs (e.g., "250,000" → 250000)
- Convert percentages to decimals (e.g., "7.5%" → 0.075)
- Handle edge cases (negative values, extremely large values, division by zero)

**Example**:
```typescript
function validateLoanInputs(inputs: LoanInputs): ValidationResult {
  const errors: string[] = [];

  if (inputs.principal <= 0) errors.push('Loan amount must be positive');
  if (inputs.principal > 100000000) errors.push('Loan amount too large');
  if (inputs.annualRate < 0 || inputs.annualRate > 0.30) errors.push('Interest rate must be 0-30%');
  if (inputs.termMonths < 1 || inputs.termMonths > 360) errors.push('Loan term must be 1-360 months');

  return { valid: errors.length === 0, errors };
}
```

**3. Golden scenario test framework**

**Purpose**: Ensure formula accuracy with known test cases

**Test structure**:
- Test data stored in JSON files (`/tests/golden-scenarios/dscr.json`)
- Each test case includes: inputs, expected outputs, tolerance (±0.01%)
- Automated tests run on every commit (CI/CD)

**Example test**:
```json
{
  "test_name": "Typical SBA 7(a) loan",
  "calculator": "business-loan-dscr",
  "inputs": {
    "principal": 250000,
    "annualRate": 0.075,
    "termMonths": 120
  },
  "expected_outputs": {
    "monthlyPayment": 2958.04,
    "totalInterest": 104964.80,
    "totalCost": 354964.80
  },
  "tolerance": 0.0001
}
```

**Test execution**:
```typescript
test('DSCR golden scenarios', () => {
  const scenarios = loadGoldenScenarios('dscr.json');

  scenarios.forEach(scenario => {
    const result = calculateDSCR(scenario.inputs);

    expect(result.monthlyPayment).toBeCloseTo(
      scenario.expected_outputs.monthlyPayment,
      scenario.tolerance
    );
  });
});
```

**Coverage target**: 10+ golden scenarios per calculator (typical cases + edge cases)

**4. Formula documentation and references**

**Documentation includes**:
- Formula name and version
- Mathematical equation (LaTeX or plain text)
- Input parameters (name, type, range, units)
- Output parameters (name, type, units)
- References (where formula comes from, e.g., "Standard amortization formula, see Wikipedia")

**Example**:
```markdown
## calculateDSCR_v1

**Purpose**: Calculate Debt Service Coverage Ratio (DSCR)

**Formula**:
DSCR = Net Operating Income / Total Debt Service

Where:
- Net Operating Income = Annual Revenue - Operating Expenses
- Total Debt Service = Annual Loan Payments (principal + interest)

**Inputs**:
- annualRevenue (number, >0, USD)
- operatingExpenses (number, >=0, USD)
- annualDebtPayment (number, >0, USD)

**Outputs**:
- dscr (number, ratio, typically 1.0-2.0)

**References**:
- SBA Lender Guide (minimum DSCR 1.25)
- https://en.wikipedia.org/wiki/Debt-service_coverage_ratio
```

### Timeline

**Weeks 1-6** (M0 phase)

**Week-by-week breakdown**:
- **Week 1-2**: Formula library architecture design, validation framework setup
- **Week 3-4**: Implement lending formulas (DSCR, amortization, SBA 7(a))
- **Week 4-5**: Implement cash flow formulas (runway, burn rate)
- **Week 5-6**: Implement profitability and valuation formulas, golden scenario testing
- **Week 6**: Code review, documentation, handoff to QA

**Milestones**:
- Week 2: Architecture approved, validation framework working
- Week 4: First formulas implemented and passing basic tests
- Week 6: All Phase 1 formulas complete, golden scenarios passing

### Risks Specific to This Workstream

**Risk 1: Formula errors**
- **Impact**: Incorrect calculations undermine product credibility
- **Mitigation**:
  - Golden scenario testing (automated, every commit)
  - External review by CFO/accountant (validate formulas against known tools)
  - Reference documentation (cite sources for formulas)
  - User feedback loop (report incorrect results)

**Risk 2: Performance bottlenecks**
- **Impact**: Calculations exceed p95 <150ms SLA (Section 1.6)
- **Mitigation**:
  - Load testing during M0 (simulate 1,000 requests/second)
  - Optimize expensive operations (avoid iterative loops, use closed-form formulas where possible)
  - Caching for repeated calculations (5-minute TTL in Redis)
  - Horizontal scaling (stateless engine, deploy multiple instances)

**Risk 3: Insufficient test coverage**
- **Impact**: Edge cases cause errors in production
- **Mitigation**:
  - Require 10+ golden scenarios per calculator (typical + edge cases)
  - Add new test cases for any bug discovered in production
  - Fuzz testing (generate random inputs, ensure no crashes)

---

## 2. UI & UX System

### Team/Owner

**Team**:
- Frontend engineering lead (owner)
- Frontend engineer (contributor)
- Designer (UX/UI design, style guide)

**Total capacity**: 2.5 FTEs

### Dependencies

**Depends on**: Calculation engine API contracts (input/output schemas)

**Timing**: Can start Week 3 (after API contracts defined) in parallel with formula implementation

### Key Deliverables

**1. React component library**

**Components** (per Section 4.1, 4.2):
- **Input components**: CurrencyInput, PercentageInput, NumberInput, SelectInput, DateInput
- **Result components**: MetricCard, ResultsGrid, ChartCard, WarningBanner
- **Scenario components**: ScenarioTabs, ScenarioActions, AddScenarioButton
- **Action components**: ExportButton, UpgradePromptModal, LoadingSpinner
- **Layout components**: CalculatorLayout, Header, Footer, Sidebar

**Technology**:
- React 18+ with TypeScript
- Tailwind CSS for styling
- Zustand for state management
- React Query for API data fetching

**Component documentation**: Storybook for component showcase and testing

**2. Design system**

**Standards** (per Section 4.3):
- **Colors**: Primary (#1E40AF), Secondary (#10B981), Warning (#F59E0B), Error (#EF4444), Gray scale (#F9FAFB to #111827)
- **Typography**: Inter font, 16px base, 1.5 line height
- **Spacing**: 8px grid (8px, 16px, 24px, 32px, 48px, 64px)
- **Shadows**: Elevation system (4 levels for depth)

**Design tokens**: CSS variables or Tailwind config for consistency

**3. Responsive layouts**

**Breakpoints**:
- **Mobile**: <640px (single column, stacked inputs/results)
- **Tablet**: 640-1024px (2 columns for some layouts)
- **Desktop**: >1024px (full layout, side-by-side inputs/results)

**Testing**:
- Test on real devices (iPhone, Android, iPad)
- Browser compatibility (Chrome, Safari, Firefox, Edge)

**4. Accessibility compliance (WCAG 2.1 AA)**

**Requirements** (per Section 4.3):
- **Keyboard navigation**: All interactive elements accessible via Tab, Enter, Escape
- **Screen reader support**: ARIA labels, semantic HTML, alt text for images
- **Color contrast**: 4.5:1 for body text, 3:1 for large text
- **Focus indicators**: Visible focus states for all interactive elements

**Testing**:
- Automated: Lighthouse accessibility audit (score >90)
- Manual: Test with screen reader (NVDA, JAWS, VoiceOver)

### Timeline

**Weeks 3-10** (M0, extends into M1)

**Week-by-week breakdown**:
- **Week 3-4**: Design system, component library setup, Storybook
- **Week 5-6**: Input and result components, responsive layouts
- **Week 7-8**: Scenario management, export modal, upgrade prompts
- **Week 9-10**: Accessibility fixes, browser testing, design polish

**Milestones**:
- Week 4: Design system approved, Storybook live
- Week 6: Core components complete (inputs, results)
- Week 8: Scenario and action components complete
- Week 10: Accessibility audit passing, ready for calculator integration

### Risks Specific to This Workstream

**Risk 1: Design inconsistencies**
- **Impact**: Poor user experience, brand confusion
- **Mitigation**:
  - Centralized design system (Tailwind config or CSS variables)
  - Design reviews before component implementation
  - Storybook for component showcase (catch inconsistencies visually)

**Risk 2: Performance on low-end devices**
- **Impact**: Slow rendering, poor mobile experience
- **Mitigation**:
  - Test on low-end devices (older iPhones, budget Android phones)
  - Code splitting (lazy load non-critical components)
  - Optimize bundle size (tree shaking, minification)
  - Lighthouse performance audit (score >80)

**Risk 3: Accessibility gaps**
- **Impact**: Legal risk, excludes users with disabilities
- **Mitigation**:
  - Automated testing (Lighthouse, axe DevTools)
  - Manual testing with screen reader
  - User testing with accessibility-focused users
  - Accessibility checklist for every component

---

## 3. Export & Reporting

### Team/Owner

**Team**:
- Backend engineer (owner, PDF/CSV/Excel generation)
- Designer (PDF template design)

**Total capacity**: 1.5 FTEs

### Dependencies

**Depends on**:
- Calculation engine outputs (data to export)
- Branding/watermarking requirements (Free vs Pro tier)

**Timing**: Can start Week 4 (after calculation engine outputs defined)

### Key Deliverables

**1. PDF generation with watermarking**

**Technology**: Puppeteer (headless Chrome for rendering HTML → PDF)

**PDF template**:
- Header: Calculator name, user tier, timestamp
- Body: Inputs table, results table, warnings (if any)
- Footer: Disclaimers (per Section 5.3), version stamp ("Calculated using DSCR formula v2")

**Watermarking** (Free tier only):
- Diagonal text across each page: "Generated by [Product Name] - Upgrade for clean exports"
- Semi-transparent gray (#6B7280, 20% opacity)
- Applied via CSS transform (rotate -45deg)

**Performance**: Target p95 <3 seconds (per Section 1.6)

**2. CSV generation with metadata headers**

**Format**:
```csv
# Business Loan + DSCR Calculator
# Version: v2
# Generated: 2025-11-17T21:30:00Z
# Tier: Free

Section,Field,Value
Inputs,Loan Amount,250000
Inputs,Interest Rate,7.5%
Inputs,Term (months),120
Outputs,Monthly Payment,2958.04
Outputs,Total Interest,104964.80
Outputs,Total Cost,354964.80
Outputs,DSCR,1.42
```

**Performance**: Target <1 second (CSV is fast, no rendering required)

**3. Excel generation (basic, single sheet)**

**Technology**: ExcelJS library

**Format**:
- Sheet 1: Inputs and outputs (similar to CSV)
- Formatted cells (currency: $#,##0.00, percentage: 0.00%)
- Bold headers, alternating row colors for readability

**Performance**: Target p95 <2 seconds

**4. Export service API (async queue for large exports)**

**Architecture**:
- **Synchronous exports** (small, <3 seconds): Return file immediately
- **Asynchronous exports** (large, >3 seconds): Queue job, return export_id, poll for completion

**Queue**: BullMQ (Redis-backed)

**API flow**:
1. `POST /api/exports` → Returns `{ export_id, status: 'queued' }`
2. Worker picks up job, generates export, uploads to S3/R2
3. `GET /api/exports/:export_id` → Returns `{ status: 'completed', file_url }` when ready

**Retry logic**: Retry up to 3 times on failure (handle transient errors)

### Timeline

**Weeks 4-8** (M0)

**Week-by-week breakdown**:
- **Week 4-5**: PDF generation setup, Puppeteer integration, basic templates
- **Week 6**: CSV and Excel generation, metadata headers
- **Week 7**: Watermarking for Free tier, async export queue
- **Week 8**: Performance optimization, error handling, testing

**Milestones**:
- Week 5: PDF generation working (basic template)
- Week 6: CSV and Excel working
- Week 8: All export formats passing performance targets, watermarking working

### Risks Specific to This Workstream

**Risk 1: PDF generation slowness**
- **Impact**: Violates p95 <3s SLA, poor user experience
- **Mitigation**:
  - Optimize PDF templates (simple layouts, minimal CSS, no large images)
  - Use async generation for complex exports (progress indicators)
  - Pre-render common templates (cache PDF structure, fill in data)
  - Add worker pool (multiple Puppeteer instances in parallel)

**Risk 2: Excel formatting complexity**
- **Impact**: Excel files look unprofessional or fail to open
- **Mitigation**:
  - Start with basic formatting (bold headers, currency/percentage formats)
  - Test Excel files in Microsoft Excel and Google Sheets (ensure compatibility)
  - Use established library (ExcelJS, well-maintained)
  - Defer advanced features (charts, multi-tab) to Phase 2

**Risk 3: S3/R2 upload failures**
- **Impact**: Exports generated but not accessible (user gets error)
- **Mitigation**:
  - Retry logic (up to 3 retries with exponential backoff)
  - Fallback storage (local disk if S3/R2 unavailable, serve from app server)
  - Monitor upload success rate (alert if <95%)

---

## 4. AI Integration

### Team/Owner

**Team**:
- Backend engineer (owner, API integration, cost management)
- Product manager (prompt design, quality assurance)

**Total capacity**: 1.5 FTEs

### Dependencies

**Depends on**:
- Calculation engine outputs (data to send to AI)
- AI provider API access (Anthropic account, API key)

**Timing**: Starts Week 8 (after calculation engine stable), extends into M1/M2

### Key Deliverables

**1. Prompt framework**

**Components** (per Section 8.2):
- **System prompts**: Define AI role, output constraints, prohibited behaviors
  - Stored in `/src/ai/prompts/` as versioned TypeScript files
  - Example: `dscr_explanation_v1.ts`, `runway_explanation_v1.ts`

- **User prompt templates**: Structured data (numeric inputs/outputs), no free-text by default
  - Template function: `(inputs, outputs) => string`
  - Example: `"The business has annual revenue of ${revenue}, operating expenses of ${expenses}..."`

**Testing**:
- Manual review of 10+ AI responses per calculator (ensure quality, tone, disclaimers)
- Automated validation (response includes disclaimer, response length 150-250 words)

**2. AI provider integration**

**Primary provider**: Anthropic Claude Sonnet 4

**Integration**:
- Anthropic SDK (`@anthropic-ai/sdk`)
- API key stored in environment variable (not hardcoded)
- Error handling (timeout, rate limit, API error)
- Fallback: Show "AI unavailable" message if API fails

**Cost tracking**:
- Log tokens used per request (input + output tokens)
- Track cost per request (~$0.006 per request, per Section 8.4)
- Monitor daily spend (alert if >$50/day, expected ~$4-6/day)

**3. Response formatting and disclaimers**

**Formatting**:
- AI response rendered as markdown (paragraphs, bullet lists)
- Disclaimers appended to every response (per Section 8.1)

**Example output**:
```
A DSCR of 1.42 means the business generates $1.42 in operating income for
every $1 in debt payments. Most lenders require a minimum DSCR of 1.25...

---

This analysis is for informational purposes only and does not constitute
financial, legal, or professional advice. Consult qualified professionals
for decisions affecting your business.
```

**4. Usage tracking and cost monitoring**

**Usage quota** (per Section 8.4):
- AI tier: 50 requests/month (hard cap)
- Track usage in `user_ai_usage` table (user_id, month, requests_used, requests_limit)
- Enforce quota before API call (reject if quota exceeded)

**Cost monitoring** (per Section 7.4):
- Dashboard: Total AI requests, total cost, cost per request
- Alert if cost >$500/month (expected ~$120/month)

### Timeline

**Weeks 8-12** (M1), expanded in M2

**Week-by-week breakdown**:
- **Week 8-9**: Anthropic API integration, basic prompt framework
- **Week 10**: DSCR and runway prompts (for M1 calculators)
- **Week 11**: Usage quota enforcement, cost tracking
- **Week 12**: Response caching, prompt optimization, testing
- **M2 expansion (Weeks 20-24)**: Prompts for remaining 6 calculators

**Milestones**:
- Week 9: Anthropic API integration working (hello world test)
- Week 10: DSCR and runway prompts generating quality responses
- Week 12: Usage quota and cost tracking working, ready for M1 launch

### Risks Specific to This Workstream

**Risk 1: AI latency**
- **Impact**: Response time >3s (violates SLA)
- **Mitigation**:
  - Hard timeout (8 seconds, fallback to "AI unavailable")
  - Prompt optimization (shorter prompts = faster response)
  - Response caching (1-hour TTL for identical inputs, 20-30% hit rate)
  - Monitor p95 latency (alert if >5 seconds)

**Risk 2: Cost overruns**
- **Impact**: AI costs exceed budget ($120/month expected)
- **Mitigation**:
  - Usage caps (50 requests/month, hard limit)
  - Prompt caching (90% cost reduction on system prompts)
  - Response caching (25% cost reduction, reuse responses)
  - Monitor daily spend (alert if >$10/day)

**Risk 3: Quality inconsistency**
- **Impact**: AI responses are inaccurate, unhelpful, or inappropriate
- **Mitigation**:
  - Manual review of 10+ responses per calculator (before launch)
  - User feedback loop (thumbs up/down on AI responses)
  - Prompt iteration based on feedback
  - Fallback to no AI if quality drops (suite_ai_enabled = false)

---

## 5. Analytics & Observability

### Team/Owner

**Team**:
- Backend engineer (owner, event tracking, APM integration)
- Data analyst (dashboard design, SQL queries)

**Total capacity**: 1.5 FTEs

### Dependencies

**Depends on**:
- Event taxonomy defined (Section 7.1)
- Analytics platform chosen (Mixpanel, Amplitude, or ClickHouse)

**Timing**: Starts Week 6 (after core events defined), ongoing instrumentation in M1/M2

### Key Deliverables

**1. Event tracking implementation**

**Standard events** (per Section 7.2):
- calculator_viewed, calculator_calculated
- scenario_created, scenario_deleted
- export_requested, export_generated, export_downloaded
- ai_narrative_requested, ai_narrative_displayed
- upgrade_prompt_shown, upgrade_prompt_clicked, upgrade_completed

**Implementation**:
- Event tracking wrapper function:
  ```typescript
  function trackEvent(eventName: string, properties: EventProperties) {
    // Validate event schema
    validateEventProperties(eventName, properties);

    // Send to analytics platform
    analytics.track(eventName, properties);

    // Log to database (for 12-month retention)
    db.logEvent(eventName, properties);
  }
  ```

**Testing**:
- Events appear in analytics platform within 5 minutes
- Event properties are valid (no null/undefined for required fields)

**2. Analytics dashboards**

**Dashboards built** (per Section 7.3):
- **Calculator Funnel**: Views → Calculations → Exports (conversion rates)
- **Tier Conversion**: Free → Pro and Pro → AI upgrade tracking
- **AI Usage**: AI requests, success rate, latency, cost
- **Export Dashboard**: Volume, formats, performance
- **Scenario Usage**: Multi-scenario adoption (Pro tier validation)

**Technology**: Metabase (open-source) or Mixpanel (SaaS)

**3. Error monitoring and alerting**

**Error tracking** (per Section 7.4):
- Calculation engine errors (formula failures, timeouts)
- Export generation errors (PDF rendering, S3 upload)
- AI service errors (API timeouts, rate limits)
- Client-side errors (JavaScript exceptions)

**Tools**: Sentry (client-side), Datadog or New Relic (server-side)

**Alerting**:
- Critical: Error rate >5% in 5 minutes → Page on-call
- Warning: Error rate 1-5% in 5 minutes → Slack notification

**4. Performance monitoring (APM integration)**

**Metrics tracked** (per Section 7.4):
- Calculation latency (p50, p95, p99)
- Export generation time (p50, p95, p99)
- AI request latency (p50, p95, p99)
- Database query times (slow query log for >100ms)

**Tools**: Datadog APM or New Relic

**Dashboards**: Real-time performance dashboard (Grafana)

### Timeline

**Weeks 6-10** (M0), ongoing instrumentation in M1/M2

**Week-by-week breakdown**:
- **Week 6-7**: Analytics platform setup, event tracking wrapper
- **Week 8**: Standard events instrumented (calculator_viewed, calculator_calculated)
- **Week 9**: Error monitoring setup (Sentry, Datadog)
- **Week 10**: Performance monitoring (APM), initial dashboards
- **M1/M2**: Add events as features launch (scenarios, AI, exports)

**Milestones**:
- Week 7: Analytics platform chosen and configured
- Week 8: First events tracked (calculator_viewed)
- Week 10: Error and performance monitoring live

### Risks Specific to This Workstream

**Risk 1: Data quality issues**
- **Impact**: Analytics dashboards show incorrect data, bad decisions
- **Mitigation**:
  - Event schema validation (reject invalid events)
  - Automated tests for event tracking (verify events fire correctly)
  - Data quality checks (daily job, alert on anomalies like 0 events)

**Risk 2: Alert fatigue**
- **Impact**: Too many alerts, team ignores critical alerts
- **Mitigation**:
  - Carefully set alert thresholds (only alert on actionable issues)
  - Use tiered alerts (critical vs warning)
  - Review alert effectiveness weekly (disable noisy alerts)

**Risk 3: Analytics platform cost**
- **Impact**: Mixpanel/Amplitude costs exceed budget
- **Mitigation**:
  - Evaluate cost before choosing platform (free tier vs paid)
  - Consider self-hosted ClickHouse (lower cost, higher complexity)
  - Monitor event volume (throttle if needed)

---

## 6. B2B/White-Label Features

### Team/Owner

**Team**:
- Backend engineer (owner, multi-tenancy, feature flags)
- Product manager (pilot customer onboarding, requirements gathering)

**Total capacity**: 1.5 FTEs

### Dependencies

**Depends on**: Core platform stable (post-M1)

**Timing**: Starts Week 16 (M2 phase), after first 2 calculators validated

### Key Deliverables

**1. Tenant configuration system**

**Tenant model**:
```typescript
interface TenantConfig {
  tenant_id: string;
  name: string;
  logo_url: string;
  primary_color: string;
  custom_domain: string | null;
  ai_enabled: boolean;
  ai_redaction_level: 'disabled' | 'strict' | 'standard' | 'enhanced';
  feature_flags: Record<string, boolean>;
  created_at: string;
  updated_at: string;
}
```

**Database table**: `tenant_configs` (per Section 8.3)

**Admin API**:
- `POST /api/admin/tenants` - Create tenant
- `PATCH /api/admin/tenants/:id` - Update tenant config
- `GET /api/admin/tenants/:id` - Get tenant config

**2. API key management and authentication**

**API keys** (B2B customers):
- Generate API key for each tenant (scoped to tenant_id)
- Format: `sk_live_abc123...` (similar to Stripe)
- Stored hashed in database (never plaintext)

**Authentication**:
- API key required for server-side calculator API calls
- Validates tenant_id and permissions
- Rate limiting per tenant (100 requests/minute)

**3. White-label embed options**

**Custom domains**:
- Customer can use `calculators.customer.com` instead of `calculators.product.com`
- CNAME configuration (customer points CNAME to our domain)
- SSL certificate (Let's Encrypt auto-provision)

**Theming**:
- Custom logo (displayed in header)
- Primary color (buttons, links)
- Custom CSS (advanced, Phase 2+)

**4. Usage tracking per tenant**

**Metrics tracked**:
- Calculations per tenant per month
- Exports per tenant per month
- AI requests per tenant per month (if AI enabled)

**Dashboard** (admin view):
- List all tenants with usage metrics
- Usage trends over time
- Top calculators per tenant

### Timeline

**Weeks 16-24** (M2)

**Week-by-week breakdown**:
- **Week 16-17**: Tenant data model, admin API
- **Week 18-19**: API key management, authentication
- **Week 20-21**: Custom domain setup, theming
- **Week 22-23**: Usage tracking, admin dashboard
- **Week 24**: Pilot customer onboarding (3 customers)

**Milestones**:
- Week 17: Tenant configuration working (can create tenant, set config)
- Week 21: White-label embed working (custom domain, branding)
- Week 24: 3 pilot customers onboarded and using calculators

### Risks Specific to This Workstream

**Risk 1: Complexity of multi-tenancy**
- **Impact**: Bugs affecting multiple customers, data leakage between tenants
- **Mitigation**:
  - Strict tenant isolation (tenant_id in all queries)
  - Automated tests for tenant isolation (verify Customer A cannot access Customer B's data)
  - Code review with security focus

**Risk 2: Custom requirements from pilots**
- **Impact**: Scope creep, custom features not scalable
- **Mitigation**:
  - Define standard features (logo, color, domain)
  - Document non-standard requests (evaluate for Phase 2)
  - Product manager filters requests (push back on one-off customizations)

**Risk 3: Custom domain SSL issues**
- **Impact**: Customer domain shows SSL warning, poor experience
- **Mitigation**:
  - Automated SSL provisioning (Let's Encrypt, Certbot)
  - Documentation for customers (how to configure CNAME)
  - Fallback to our domain if SSL fails (graceful degradation)

---

## Summary

Six workstreams run in parallel to deliver the CFO Business Intelligence Calculator Suite:

**1. Calculation Engine & Formula Library** (Weeks 1-6):
- Foundation for all calculators, blocks M1
- Risks: Formula errors, performance bottlenecks

**2. UI & UX System** (Weeks 3-10):
- Component library, design system, accessibility
- Risks: Design inconsistencies, performance on low-end devices

**3. Export & Reporting** (Weeks 4-8):
- PDF, CSV, Excel generation with watermarking
- Risks: PDF slowness, Excel formatting complexity

**4. AI Integration** (Weeks 8-12, expanded in M2):
- Prompt framework, Anthropic API, usage quota, cost tracking
- Risks: AI latency, cost overruns, quality inconsistency

**5. Analytics & Observability** (Weeks 6-10, ongoing):
- Event tracking, dashboards, error monitoring, APM
- Risks: Data quality issues, alert fatigue

**6. B2B/White-Label Features** (Weeks 16-24):
- Tenant configuration, API keys, custom domains, theming
- Risks: Multi-tenancy complexity, custom requirements, SSL issues

**Critical path**: Calculation Engine → UI Components → First 2 Calculators (M1) → Remaining 6 Calculators (M2)
